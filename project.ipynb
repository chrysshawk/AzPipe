{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import azureml.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current workspace: ml-workspace\n"
     ]
    }
   ],
   "source": [
    "# assign current workspace\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "print('Current workspace:', ws.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current datastore: workspaceblobstore\n"
     ]
    }
   ],
   "source": [
    "# assign datastore\n",
    "ds = ws.get_default_datastore()\n",
    "print('Current datastore:', ds.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading ./data/heart.csv\n",
      "Uploaded ./data/heart.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    },
    {
     "data": {
      "text/plain": "$AZUREML_DATAREFERENCE_39dde4d9e65f4c928ee8a8ff32f49928"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload the file to blob\n",
    "ds.upload_files(files=['./data/heart.csv'], target_path='data-heart/', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already registered.\n"
     ]
    }
   ],
   "source": [
    "# register the dataset\n",
    "from azureml.core import Dataset\n",
    "\n",
    "# name of our dataset\n",
    "ds_name = 'heart dataset'\n",
    "\n",
    "if 'heart dataset' not in ws.datasets:\n",
    "    # create tabular dataset from the data\n",
    "    heart_data = Dataset.Tabular.from_delimited_files(path=(ds, 'data-heart/*.csv'))\n",
    "\n",
    "    # register the dataset\n",
    "    try:\n",
    "        heart_data = heart_data.register(workspace=ws,\n",
    "                                         name = ds_name,\n",
    "                                         description= 'Heart Attach Data',\n",
    "                                         tags = {'format' : 'csv'},\n",
    "                                         create_new_version = True)\n",
    "        print('Dataset %s registered with version %i.'%(heart_data.name, str(heart_data.version)))\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "else:\n",
    "    print('Dataset already registered.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./heart_pipeline\n"
     ]
    }
   ],
   "source": [
    "# Create a local folder for the pipeline step files\n",
    "\n",
    "import os\n",
    "experiment_folder = './heart_pipeline'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "print(experiment_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Step 1: Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./heart_pipeline/01_prep_heart.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/01_prep_heart.py\n",
    "# Import libraries\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from azureml.core import Run\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input-data', type=str, dest='raw_dataset_id',\n",
    "                    help='raw dataset')\n",
    "parser.add_argument('--prepped-data', type=str, dest='prepped_data',\n",
    "                    default='prepped_data',\n",
    "                    help='Folder for results')\n",
    "args = parser.parse_args()\n",
    "save_folder = args.prepped_data\n",
    "\n",
    "# Get experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Load data\n",
    "print('Loading Data...')\n",
    "heart_df = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
    "\n",
    "# Log raw dataset details\n",
    "run.log('Raw rows', heart_df.shape[0])\n",
    "run.log('Raw columns', heart_df.shape[1])\n",
    "\n",
    "# Drop NAs\n",
    "heart_df = heart_df.dropna()\n",
    "\n",
    "# Modifying prediction label\n",
    "heart_df.rename(columns={'output' : 'heart_attack'}, inplace=True)\n",
    "\n",
    "# Change sex categorical feature to dummies\n",
    "sex_type = pd.get_dummies(data=heart_df['sex'])\n",
    "sex_type.columns = ['Male', 'Female']\n",
    "chest_pain = pd.get_dummies(data=heart_df['cp'])\n",
    "chest_pain.columns = ['Chest Pain 1', 'Chest Pain 2', 'Chest Pain 3',\n",
    "                      'Chest Pain 4']\n",
    "ex_angina = pd.get_dummies(data=heart_df['exng'])\n",
    "ex_angina.columns = ['Exercise Angina: No', 'Exercise Angina: Yes']\n",
    "slp_type = pd.get_dummies(data=heart_df['slp'])\n",
    "slp_type.columns = ['Slope 1', 'Slope 2', 'Slope 3']\n",
    "caa_type = pd.get_dummies(data=heart_df['caa'])\n",
    "caa_type.columns = ['CAA 1', 'CAA 2', 'CAA 3', 'CAA 4', 'CAA 5']\n",
    "thall_type = pd.get_dummies(data=heart_df['thall'])\n",
    "thall_type.columns = ['Thall 1', 'Thall 2', 'Thall 3', 'Thall 4']\n",
    "\n",
    "\n",
    "# Joining and removing modified columns\n",
    "heart_df = pd.concat([sex_type, chest_pain, ex_angina, slp_type,\n",
    "                      caa_type, thall_type, heart_df],\n",
    "                     axis=1)\n",
    "heart_df.drop(labels=['sex', 'cp', 'exng', 'slp', 'caa', 'thall'], \n",
    "              axis=1, inplace=True)\n",
    "\n",
    "# Normalize numeric columns\n",
    "scaler = MinMaxScaler()\n",
    "norm_cols = ['age', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh',\n",
    "                'oldpeak']\n",
    "heart_df[norm_cols] = scaler.fit_transform(heart_df[norm_cols])\n",
    "\n",
    "# Log prepped dataset details\n",
    "run.log('Prepped rows', heart_df.shape[0])\n",
    "run.log('Prepped columns', heart_df.shape[1])\n",
    "\n",
    "# Save the prepped data\n",
    "print('Saving Data...')\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "save_path = os.path.join(save_folder, 'heartdata_prepped.csv')\n",
    "heart_df.to_csv(save_path, index=False, header=True)\n",
    "\n",
    "# End the run\n",
    "run.complete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Step 2: Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/02_train_heart.py\n",
    "\n",
    "# Import libraries\n",
    "from azureml.core import Run, Model\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--training-data',\n",
    "                    type=str,\n",
    "                    dest='training_data',\n",
    "                    help='training_data')\n",
    "args = parser.parse_args()\n",
    "training_data = args.training_data\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Load the data\n",
    "print('Loading Data...')\n",
    "file_path = os.path.join(training_data, 'heartdata_prepped.csv')\n",
    "heart = pd.read_csv(file_path)\n",
    "\n",
    "# Separate features from labels\n",
    "X, y = heart.iloc[:,0:-1].values, heart.iloc[:,-1]\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Train model\n",
    "print('Training model...')\n",
    "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# Calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test, y_scores[:,1])\n",
    "print('AUC:', auc)\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.plot([0,1], [0,1], 'k--') # diagonal 50% line\n",
    "plt.plot(fpr, tpr, 'r--')\n",
    "plt.xlabel('FP rate')\n",
    "plt.ylabel('TP rate')\n",
    "plt.title('ROC Curve')\n",
    "run.log_image(name='ROC', plot=fig)\n",
    "plt.show()\n",
    "\n",
    "# Save trained model \n",
    "print('Saving model...')\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "model_file = os.path.join('./outputs', 'heart_model.pkl')\n",
    "joblib.dump(value=model, filename=model_file)\n",
    "\n",
    "# Register model\n",
    "print('Registering model...')\n",
    "Model.register(workspace = run.experiment.workspace,\n",
    "               model_path = model_file,\n",
    "               model_name = 'heart_model',\n",
    "               tags = {'Context' : 'Pipeline',\n",
    "                       'Purpose' : 'DP100'},\n",
    "               properties = {'AUC' : np.float(auc),\n",
    "                             'Accuracy' : np.float(acc)})\n",
    "               \n",
    "run.complete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep compute environment\n",
    "\n",
    "This is only necessary if a compute environment is required (i.e. it will not run locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, using it.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = 'chryssCluster'\n",
    "\n",
    "try:\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws,\n",
    "                                     name=cluster_name)\n",
    "    print('Found existing cluster, using it.')\n",
    "except:\n",
    "    # It does not exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(\n",
    "            vm_size='STANDARD_DS11_V2',\n",
    "            max_nodes=2)\n",
    "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create conda environment config\n",
    "\n",
    "This will be installed on the compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./heart_pipeline/heart_experiment_env.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/heart_experiment_env.yml\n",
    "name: heart_experiment_env\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- ipykernel\n",
    "- matplotlib\n",
    "- pandas\n",
    "- pip\n",
    "- pip:\n",
    "    - azureml-defaults\n",
    "    - pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create run configuration: Environment & Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "heart_experiment_env = Environment.from_conda_specification(\n",
    "    'heart_experiment_env', experiment_folder + '/heart_experiment_env.yml')\n",
    "\n",
    "# Register environment\n",
    "heart_experiment_env.register(workspace=ws)\n",
    "registered_env = Environment.get(ws, 'heart_experiment_env')\n",
    "\n",
    "# Create RunConfig object for pipeline\n",
    "pipeline_run_config = RunConfiguration()\n",
    "pipeline_run_config.target = pipeline_cluster\n",
    "pipeline_run_config.environment = registered_env\n",
    "\n",
    "print('Run configuration created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps defined.\n",
      "Pipeline is built.\n"
     ]
    }
   ],
   "source": [
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "# Get the dataset\n",
    "heart_ds = ws.datasets.get('heart dataset')\n",
    "\n",
    "# Create OutputFileDatasetConfig (temp data)\n",
    "prepped_data = OutputFileDatasetConfig('prepped_data')\n",
    "\n",
    "# Create steps\n",
    "prep_step = PythonScriptStep(name = 'Prepare Data',\n",
    "                                source_directory = experiment_folder,\n",
    "                                script_name = '01_prep_heart.py',\n",
    "                                arguments = ['--input-data', heart_ds.as_named_input('raw_data'),\n",
    "                                             '--prepped-data', prepped_data],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "train_step = PythonScriptStep(name = 'Train and Register Model',\n",
    "                                 source_directory = experiment_folder,\n",
    "                                 script_name = '02_train_heart.py',\n",
    "                                 arguments = ['--training-data',\n",
    "                                              prepped_data.as_input()],\n",
    "                                 compute_target = pipeline_cluster,\n",
    "                                 runconfig = pipeline_run_config,\n",
    "                                 allow_reuse = True)\n",
    "\n",
    "print('Pipeline steps defined.')\n",
    "\n",
    "# Construct pipeline\n",
    "pipeline_steps = [prep_step, train_step]\n",
    "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
    "print('Pipeline is built.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Pipeline and Run as Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step Prepare Data [3d38a8da][14011a9c-3546-4de6-88a4-0c178f2b8032], (This step will run and generate new outputs)Created step Train and Register Model [ac4d1da7][e103e3d6-741e-422d-9d56-0548cb2240d2], (This step will run and generate new outputs)\n",
      "\n",
      "Submitted PipelineRun 1b3c7074-1cf6-4bb2-8aaa-48fdb49d035f\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Create an experiment and run pipeline\n",
    "experiment = Experiment(workspace=ws, name='heart_pipeline')\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n",
    "print('Pipeline submitted for execution.')\n",
    "RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion(show_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('ch_azure': conda)",
   "language": "python",
   "name": "python3810jvsc74a57bd05a32c7bc2300999b8fb3e0da655551b24b48d1e812c311f299692aa0706723be"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "5a32c7bc2300999b8fb3e0da655551b24b48d1e812c311f299692aa0706723be"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}